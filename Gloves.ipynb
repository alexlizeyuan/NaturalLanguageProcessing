{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8360872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in dictionary= 10\n",
      "Dictionary is =  {'university': 1, 'science': 2, 'pennsylvnia': 3, 'processing': 4, 'harrisburg': 5, 'state': 6, 'cmpsc597': 7, 'computer': 8, 'language': 9, 'natrual': 10}\n",
      "Dense vector for first word is =>  [-0.11235     0.090656    0.45074999  0.36649001  0.0030079  -0.58425999\n",
      "  0.11999    -0.40391001  0.77354997 -0.97389001 -0.10206     0.094308\n",
      "  0.47667     0.11135     0.63183999  0.81681001  0.18341     0.075378\n",
      "  0.15082    -0.067122    0.025803   -0.50348997  0.19478001  0.85101002\n",
      " -0.68379003  0.90408999 -0.14044    -0.030979   -0.19029    -0.74045998\n",
      "  0.66946    -0.43092999 -0.39607     0.90126997 -0.82384998  0.067228\n",
      "  0.19033     0.28380999 -0.27217999 -0.13001999  0.28455001 -0.26232001\n",
      " -0.51340997  0.52877998 -0.41376001 -0.26901001  0.16203     0.74839997\n",
      " -0.13806    -0.079428  ]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "  \n",
    "# x = {'text', 'the', 'leader', 'prime',\n",
    "#      'natural', 'language'}\n",
    "\n",
    "x = {'computer', 'science', 'natrual', 'language',\n",
    "     'processing', 'CMPSC597','Pennsylvnia','State','University','Harrisburg'}\n",
    "  \n",
    "# create the dict.\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x)\n",
    "  \n",
    "# number of unique words in dict.\n",
    "print(\"Number of unique words in dictionary=\", \n",
    "      len(tokenizer.word_index))\n",
    "print(\"Dictionary is = \", tokenizer.word_index)\n",
    "  \n",
    "# download glove and unzip it in Notebook.\n",
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip glove*.zip\n",
    "  \n",
    "# vocab: 'the': 1, mapping of words with\n",
    "# integers in seq. 1,2,3..\n",
    "# embedding: 1->dense vector\n",
    "def embedding_for_vocab(filepath, word_index,\n",
    "                        embedding_dim):\n",
    "    vocab_size = len(word_index) + 1\n",
    "      \n",
    "    # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix_vocab = np.zeros((vocab_size,\n",
    "                                       embedding_dim))\n",
    "  \n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                embedding_matrix_vocab[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "  \n",
    "    return embedding_matrix_vocab\n",
    "  \n",
    "  \n",
    "# matrix for vocab: word_index\n",
    "embedding_dim = 50\n",
    "embedding_matrix_vocab = embedding_for_vocab(\n",
    "    './glove.6B.300d.txt', tokenizer.word_index,\n",
    "  embedding_dim)\n",
    "  \n",
    "print(\"Dense vector for first word is => \",\n",
    "      embedding_matrix_vocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cad5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
